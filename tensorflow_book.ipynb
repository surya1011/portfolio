{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data = pd.read_csv('pulsarstarscsvzip-36616/pulsar_stars.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    16259\n",
       "1     1639\n",
       "Name: target_class, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_data.target_class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " Mean of the integrated profile                  0\n",
       " Standard deviation of the integrated profile    0\n",
       " Excess kurtosis of the integrated profile       0\n",
       " Skewness of the integrated profile              0\n",
       " Mean of the DM-SNR curve                        0\n",
       " Standard deviation of the DM-SNR curve          0\n",
       " Excess kurtosis of the DM-SNR curve             0\n",
       " Skewness of the DM-SNR curve                    0\n",
       "target_class                                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean of the integrated profile</th>\n",
       "      <th>Standard deviation of the integrated profile</th>\n",
       "      <th>Excess kurtosis of the integrated profile</th>\n",
       "      <th>Skewness of the integrated profile</th>\n",
       "      <th>Mean of the DM-SNR curve</th>\n",
       "      <th>Standard deviation of the DM-SNR curve</th>\n",
       "      <th>Excess kurtosis of the DM-SNR curve</th>\n",
       "      <th>Skewness of the DM-SNR curve</th>\n",
       "      <th>target_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>17898.000000</td>\n",
       "      <td>17898.000000</td>\n",
       "      <td>17898.000000</td>\n",
       "      <td>17898.000000</td>\n",
       "      <td>17898.000000</td>\n",
       "      <td>17898.000000</td>\n",
       "      <td>17898.000000</td>\n",
       "      <td>17898.000000</td>\n",
       "      <td>17898.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>111.079968</td>\n",
       "      <td>46.549532</td>\n",
       "      <td>0.477857</td>\n",
       "      <td>1.770279</td>\n",
       "      <td>12.614400</td>\n",
       "      <td>26.326515</td>\n",
       "      <td>8.303556</td>\n",
       "      <td>104.857709</td>\n",
       "      <td>0.091574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>25.652935</td>\n",
       "      <td>6.843189</td>\n",
       "      <td>1.064040</td>\n",
       "      <td>6.167913</td>\n",
       "      <td>29.472897</td>\n",
       "      <td>19.470572</td>\n",
       "      <td>4.506092</td>\n",
       "      <td>106.514540</td>\n",
       "      <td>0.288432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.812500</td>\n",
       "      <td>24.772042</td>\n",
       "      <td>-1.876011</td>\n",
       "      <td>-1.791886</td>\n",
       "      <td>0.213211</td>\n",
       "      <td>7.370432</td>\n",
       "      <td>-3.139270</td>\n",
       "      <td>-1.976976</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>100.929688</td>\n",
       "      <td>42.376018</td>\n",
       "      <td>0.027098</td>\n",
       "      <td>-0.188572</td>\n",
       "      <td>1.923077</td>\n",
       "      <td>14.437332</td>\n",
       "      <td>5.781506</td>\n",
       "      <td>34.960504</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>115.078125</td>\n",
       "      <td>46.947479</td>\n",
       "      <td>0.223240</td>\n",
       "      <td>0.198710</td>\n",
       "      <td>2.801839</td>\n",
       "      <td>18.461316</td>\n",
       "      <td>8.433515</td>\n",
       "      <td>83.064556</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>127.085938</td>\n",
       "      <td>51.023202</td>\n",
       "      <td>0.473325</td>\n",
       "      <td>0.927783</td>\n",
       "      <td>5.464256</td>\n",
       "      <td>28.428104</td>\n",
       "      <td>10.702959</td>\n",
       "      <td>139.309331</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>192.617188</td>\n",
       "      <td>98.778911</td>\n",
       "      <td>8.069522</td>\n",
       "      <td>68.101622</td>\n",
       "      <td>223.392140</td>\n",
       "      <td>110.642211</td>\n",
       "      <td>34.539844</td>\n",
       "      <td>1191.000837</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Mean of the integrated profile  \\\n",
       "count                     17898.000000   \n",
       "mean                        111.079968   \n",
       "std                          25.652935   \n",
       "min                           5.812500   \n",
       "25%                         100.929688   \n",
       "50%                         115.078125   \n",
       "75%                         127.085938   \n",
       "max                         192.617188   \n",
       "\n",
       "        Standard deviation of the integrated profile  \\\n",
       "count                                   17898.000000   \n",
       "mean                                       46.549532   \n",
       "std                                         6.843189   \n",
       "min                                        24.772042   \n",
       "25%                                        42.376018   \n",
       "50%                                        46.947479   \n",
       "75%                                        51.023202   \n",
       "max                                        98.778911   \n",
       "\n",
       "        Excess kurtosis of the integrated profile  \\\n",
       "count                                17898.000000   \n",
       "mean                                     0.477857   \n",
       "std                                      1.064040   \n",
       "min                                     -1.876011   \n",
       "25%                                      0.027098   \n",
       "50%                                      0.223240   \n",
       "75%                                      0.473325   \n",
       "max                                      8.069522   \n",
       "\n",
       "        Skewness of the integrated profile   Mean of the DM-SNR curve  \\\n",
       "count                         17898.000000               17898.000000   \n",
       "mean                              1.770279                  12.614400   \n",
       "std                               6.167913                  29.472897   \n",
       "min                              -1.791886                   0.213211   \n",
       "25%                              -0.188572                   1.923077   \n",
       "50%                               0.198710                   2.801839   \n",
       "75%                               0.927783                   5.464256   \n",
       "max                              68.101622                 223.392140   \n",
       "\n",
       "        Standard deviation of the DM-SNR curve  \\\n",
       "count                             17898.000000   \n",
       "mean                                 26.326515   \n",
       "std                                  19.470572   \n",
       "min                                   7.370432   \n",
       "25%                                  14.437332   \n",
       "50%                                  18.461316   \n",
       "75%                                  28.428104   \n",
       "max                                 110.642211   \n",
       "\n",
       "        Excess kurtosis of the DM-SNR curve   Skewness of the DM-SNR curve  \\\n",
       "count                          17898.000000                   17898.000000   \n",
       "mean                               8.303556                     104.857709   \n",
       "std                                4.506092                     106.514540   \n",
       "min                               -3.139270                      -1.976976   \n",
       "25%                                5.781506                      34.960504   \n",
       "50%                                8.433515                      83.064556   \n",
       "75%                               10.702959                     139.309331   \n",
       "max                               34.539844                    1191.000837   \n",
       "\n",
       "       target_class  \n",
       "count  17898.000000  \n",
       "mean       0.091574  \n",
       "std        0.288432  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000000  \n",
       "max        1.000000  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = total_data.values[:,:-1]\n",
    "y = total_data.values[:,-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=44)\n",
    "X_val, X_tes, y_val, y_tes = train_test_split(X_test, y_test, test_size=0.5, stratify=y_test, random_state=44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12528, 8)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2685, 8)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2685, 8)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tensorflow.keras.layers' from 'C:\\\\Users\\\\00004891\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\lib\\\\site-packages\\\\tensorflow\\\\_api\\\\v1\\\\keras\\\\layers\\\\__init__.py'>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12528 samples, validate on 2685 samples\n",
      "Epoch 1/20\n",
      "12528/12528 [==============================] - 2s 194us/sample - loss: 0.1538 - acc: 0.9644 - val_loss: 0.1002 - val_acc: 0.9721\n",
      "Epoch 2/20\n",
      "12528/12528 [==============================] - 2s 123us/sample - loss: 0.1105 - acc: 0.9716 - val_loss: 0.0805 - val_acc: 0.9773\n",
      "Epoch 3/20\n",
      "12528/12528 [==============================] - 2s 125us/sample - loss: 0.1060 - acc: 0.9720 - val_loss: 0.0821 - val_acc: 0.9750\n",
      "Epoch 4/20\n",
      "12528/12528 [==============================] - 2s 126us/sample - loss: 0.0985 - acc: 0.9733 - val_loss: 0.0874 - val_acc: 0.9758\n",
      "Epoch 5/20\n",
      "12528/12528 [==============================] - 2s 131us/sample - loss: 0.0970 - acc: 0.9738 - val_loss: 0.0896 - val_acc: 0.9750\n",
      "Epoch 6/20\n",
      "12528/12528 [==============================] - 2s 129us/sample - loss: 0.0919 - acc: 0.9749 - val_loss: 0.0784 - val_acc: 0.9769\n",
      "Epoch 7/20\n",
      "12528/12528 [==============================] - 2s 126us/sample - loss: 0.0900 - acc: 0.9750 - val_loss: 0.0784 - val_acc: 0.9773\n",
      "Epoch 8/20\n",
      "12528/12528 [==============================] - 2s 126us/sample - loss: 0.0869 - acc: 0.9744 - val_loss: 0.0888 - val_acc: 0.9732\n",
      "Epoch 9/20\n",
      "12528/12528 [==============================] - 2s 127us/sample - loss: 0.0885 - acc: 0.9757 - val_loss: 0.0782 - val_acc: 0.9777\n",
      "Epoch 10/20\n",
      "12528/12528 [==============================] - 2s 127us/sample - loss: 0.0854 - acc: 0.9755 - val_loss: 0.0792 - val_acc: 0.9769\n",
      "Epoch 11/20\n",
      "12528/12528 [==============================] - 2s 124us/sample - loss: 0.0833 - acc: 0.9757 - val_loss: 0.0797 - val_acc: 0.9765\n",
      "Epoch 12/20\n",
      "12528/12528 [==============================] - 2s 121us/sample - loss: 0.0856 - acc: 0.9746 - val_loss: 0.0786 - val_acc: 0.9780\n",
      "Epoch 13/20\n",
      "12528/12528 [==============================] - 1s 118us/sample - loss: 0.0825 - acc: 0.9748 - val_loss: 0.0824 - val_acc: 0.9728\n",
      "Epoch 14/20\n",
      "12528/12528 [==============================] - 2s 121us/sample - loss: 0.0833 - acc: 0.9760 - val_loss: 0.0798 - val_acc: 0.9758\n",
      "Epoch 15/20\n",
      "12528/12528 [==============================] - 2s 126us/sample - loss: 0.0819 - acc: 0.9753 - val_loss: 0.0783 - val_acc: 0.9758\n",
      "Epoch 16/20\n",
      "12528/12528 [==============================] - 2s 124us/sample - loss: 0.0819 - acc: 0.9758 - val_loss: 0.0788 - val_acc: 0.9765\n",
      "Epoch 17/20\n",
      "12528/12528 [==============================] - 1s 117us/sample - loss: 0.0816 - acc: 0.9761 - val_loss: 0.0777 - val_acc: 0.9769\n",
      "Epoch 18/20\n",
      "12528/12528 [==============================] - 2s 122us/sample - loss: 0.0825 - acc: 0.9764 - val_loss: 0.0765 - val_acc: 0.9773\n",
      "Epoch 19/20\n",
      "12528/12528 [==============================] - 2s 121us/sample - loss: 0.0796 - acc: 0.9770 - val_loss: 0.0795 - val_acc: 0.9769\n",
      "Epoch 20/20\n",
      "12528/12528 [==============================] - 2s 122us/sample - loss: 0.0809 - acc: 0.9764 - val_loss: 0.0785 - val_acc: 0.9777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2897c437a20>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(Dense(64, input_shape=(8,), activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer ='adam',loss='binary_crossentropy', metrics =['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=16, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2685/2685 [==============================] - 0s 39us/sample - loss: 0.0883 - acc: 0.9750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08830942188006317, 0.9750466]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_tes, y_tes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_hidden_layers, num_hidden_units, dropout=False):\n",
    "    hidden_layers = [Dense(num_hidden_units, input_shape=(8,), activation='relu') for _ in range(num_hidden_layers)]\n",
    "    model = tf.keras.Sequential(hidden_layers)\n",
    "    model.add(Dense(1, activation='sigmoid'))   #Adding the output layer\n",
    "    model.compile(optimizer ='adam',loss='binary_crossentropy', metrics =['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12528 samples, validate on 2685 samples\n",
      "Epoch 1/20\n",
      "12528/12528 [==============================] - 2s 168us/sample - loss: 0.1719 - acc: 0.9574 - val_loss: 0.0948 - val_acc: 0.9724\n",
      "Epoch 2/20\n",
      "12528/12528 [==============================] - 2s 124us/sample - loss: 0.1022 - acc: 0.9711 - val_loss: 0.0853 - val_acc: 0.9758\n",
      "Epoch 3/20\n",
      "12528/12528 [==============================] - 2s 125us/sample - loss: 0.0955 - acc: 0.9722 - val_loss: 0.0930 - val_acc: 0.9758\n",
      "Epoch 4/20\n",
      "12528/12528 [==============================] - 1s 118us/sample - loss: 0.0942 - acc: 0.9733 - val_loss: 0.0896 - val_acc: 0.9736\n",
      "Epoch 5/20\n",
      "12528/12528 [==============================] - 1s 114us/sample - loss: 0.0929 - acc: 0.9721 - val_loss: 0.0805 - val_acc: 0.9773\n",
      "Epoch 6/20\n",
      "12528/12528 [==============================] - 1s 106us/sample - loss: 0.0898 - acc: 0.9743 - val_loss: 0.0823 - val_acc: 0.9784\n",
      "Epoch 7/20\n",
      "12528/12528 [==============================] - 1s 115us/sample - loss: 0.0891 - acc: 0.9744 - val_loss: 0.0823 - val_acc: 0.9769\n",
      "Epoch 8/20\n",
      "12528/12528 [==============================] - 1s 115us/sample - loss: 0.0900 - acc: 0.9753 - val_loss: 0.1000 - val_acc: 0.9739\n",
      "Epoch 9/20\n",
      "12528/12528 [==============================] - 1s 115us/sample - loss: 0.0887 - acc: 0.9747 - val_loss: 0.0783 - val_acc: 0.9795\n",
      "Epoch 10/20\n",
      "12528/12528 [==============================] - 1s 114us/sample - loss: 0.0860 - acc: 0.9748 - val_loss: 0.0821 - val_acc: 0.9784\n",
      "Epoch 11/20\n",
      "12528/12528 [==============================] - 1s 115us/sample - loss: 0.0850 - acc: 0.9764 - val_loss: 0.0785 - val_acc: 0.9769\n",
      "Epoch 12/20\n",
      "12528/12528 [==============================] - 1s 115us/sample - loss: 0.0843 - acc: 0.9757 - val_loss: 0.0770 - val_acc: 0.9769\n",
      "Epoch 13/20\n",
      "12528/12528 [==============================] - 1s 115us/sample - loss: 0.0837 - acc: 0.9747 - val_loss: 0.0799 - val_acc: 0.9769\n",
      "Epoch 14/20\n",
      "12528/12528 [==============================] - 1s 115us/sample - loss: 0.0848 - acc: 0.9757 - val_loss: 0.0779 - val_acc: 0.9780\n",
      "Epoch 15/20\n",
      "12528/12528 [==============================] - 1s 114us/sample - loss: 0.0828 - acc: 0.9756 - val_loss: 0.0797 - val_acc: 0.9750\n",
      "Epoch 16/20\n",
      "12528/12528 [==============================] - 1s 114us/sample - loss: 0.0820 - acc: 0.9768 - val_loss: 0.0801 - val_acc: 0.9758\n",
      "Epoch 17/20\n",
      "12528/12528 [==============================] - 1s 114us/sample - loss: 0.0821 - acc: 0.9760 - val_loss: 0.0769 - val_acc: 0.9780\n",
      "Epoch 18/20\n",
      "12528/12528 [==============================] - 1s 113us/sample - loss: 0.0821 - acc: 0.9761 - val_loss: 0.0779 - val_acc: 0.9784\n",
      "Epoch 19/20\n",
      "12528/12528 [==============================] - 1s 115us/sample - loss: 0.0815 - acc: 0.9767 - val_loss: 0.0788 - val_acc: 0.9784\n",
      "Epoch 20/20\n",
      "12528/12528 [==============================] - 1s 115us/sample - loss: 0.0831 - acc: 0.9767 - val_loss: 0.0763 - val_acc: 0.9773\n",
      "Train on 12528 samples, validate on 2685 samples\n",
      "Epoch 1/20\n",
      "12528/12528 [==============================] - 2s 135us/sample - loss: 0.2484 - acc: 0.9579 - val_loss: 0.1299 - val_acc: 0.9628\n",
      "Epoch 2/20\n",
      "12528/12528 [==============================] - 1s 93us/sample - loss: 0.1083 - acc: 0.9712 - val_loss: 0.1037 - val_acc: 0.9721\n",
      "Epoch 3/20\n",
      "12528/12528 [==============================] - 1s 93us/sample - loss: 0.1046 - acc: 0.9720 - val_loss: 0.0906 - val_acc: 0.9754\n",
      "Epoch 4/20\n",
      "12528/12528 [==============================] - 1s 93us/sample - loss: 0.1038 - acc: 0.9729 - val_loss: 0.0872 - val_acc: 0.9758\n",
      "Epoch 5/20\n",
      "12528/12528 [==============================] - 1s 92us/sample - loss: 0.1035 - acc: 0.9733 - val_loss: 0.1030 - val_acc: 0.9750\n",
      "Epoch 6/20\n",
      "12528/12528 [==============================] - 1s 92us/sample - loss: 0.0973 - acc: 0.9738 - val_loss: 0.0880 - val_acc: 0.9739\n",
      "Epoch 7/20\n",
      "12528/12528 [==============================] - 1s 93us/sample - loss: 0.0997 - acc: 0.9733 - val_loss: 0.0839 - val_acc: 0.9788\n",
      "Epoch 8/20\n",
      "12528/12528 [==============================] - 1s 94us/sample - loss: 0.0956 - acc: 0.9732 - val_loss: 0.0970 - val_acc: 0.9721\n",
      "Epoch 9/20\n",
      "12528/12528 [==============================] - 1s 92us/sample - loss: 0.0962 - acc: 0.9730 - val_loss: 0.0973 - val_acc: 0.9762\n",
      "Epoch 10/20\n",
      "12528/12528 [==============================] - 1s 93us/sample - loss: 0.0965 - acc: 0.9742 - val_loss: 0.0908 - val_acc: 0.9736\n",
      "Epoch 11/20\n",
      "12528/12528 [==============================] - 1s 93us/sample - loss: 0.0939 - acc: 0.9737 - val_loss: 0.0942 - val_acc: 0.9762\n",
      "Epoch 12/20\n",
      "12528/12528 [==============================] - 1s 93us/sample - loss: 0.0912 - acc: 0.9754 - val_loss: 0.0780 - val_acc: 0.9769\n",
      "Epoch 13/20\n",
      "12528/12528 [==============================] - 1s 92us/sample - loss: 0.0885 - acc: 0.9775 - val_loss: 0.0825 - val_acc: 0.9784\n",
      "Epoch 14/20\n",
      "12528/12528 [==============================] - 1s 92us/sample - loss: 0.0915 - acc: 0.9750 - val_loss: 0.0837 - val_acc: 0.9784\n",
      "Epoch 15/20\n",
      "12528/12528 [==============================] - 1s 92us/sample - loss: 0.0910 - acc: 0.9752 - val_loss: 0.0871 - val_acc: 0.9788\n",
      "Epoch 16/20\n",
      "12528/12528 [==============================] - 1s 93us/sample - loss: 0.0891 - acc: 0.9756 - val_loss: 0.0825 - val_acc: 0.9788\n",
      "Epoch 17/20\n",
      "12528/12528 [==============================] - 1s 93us/sample - loss: 0.0882 - acc: 0.9763 - val_loss: 0.0810 - val_acc: 0.9784\n",
      "Epoch 18/20\n",
      "12528/12528 [==============================] - 1s 93us/sample - loss: 0.0879 - acc: 0.9768 - val_loss: 0.0807 - val_acc: 0.9773\n",
      "Epoch 19/20\n",
      "12528/12528 [==============================] - 1s 92us/sample - loss: 0.0884 - acc: 0.9756 - val_loss: 0.0970 - val_acc: 0.9736\n",
      "Epoch 20/20\n",
      "12528/12528 [==============================] - 1s 92us/sample - loss: 0.0854 - acc: 0.9761 - val_loss: 0.0782 - val_acc: 0.9769\n",
      "Train on 12528 samples, validate on 2685 samples\n",
      "Epoch 1/20\n",
      "12528/12528 [==============================] - 2s 190us/sample - loss: 14.4828 - acc: 0.0916 - val_loss: 14.4817 - val_acc: 0.0916\n",
      "Epoch 2/20\n",
      "12528/12528 [==============================] - 2s 139us/sample - loss: 14.4828 - acc: 0.0916 - val_loss: 14.4817 - val_acc: 0.0916\n",
      "Epoch 3/20\n",
      "12528/12528 [==============================] - 2s 140us/sample - loss: 14.4828 - acc: 0.0916 - val_loss: 14.4817 - val_acc: 0.0916\n",
      "Epoch 4/20\n",
      "12528/12528 [==============================] - 2s 134us/sample - loss: 14.4828 - acc: 0.0916 - val_loss: 14.4817 - val_acc: 0.0916\n",
      "Epoch 5/20\n",
      "12528/12528 [==============================] - 2s 133us/sample - loss: 14.4828 - acc: 0.0916 - val_loss: 14.4817 - val_acc: 0.0916\n",
      "Epoch 6/20\n",
      "12528/12528 [==============================] - 2s 134us/sample - loss: 14.4828 - acc: 0.0916 - val_loss: 14.4817 - val_acc: 0.0916\n",
      "Epoch 7/20\n",
      "12528/12528 [==============================] - 2s 131us/sample - loss: 14.4828 - acc: 0.0916 - val_loss: 14.4817 - val_acc: 0.0916\n",
      "Epoch 8/20\n",
      "12528/12528 [==============================] - 2s 128us/sample - loss: 14.4828 - acc: 0.0916 - val_loss: 14.4817 - val_acc: 0.0916\n",
      "Epoch 9/20\n",
      "12528/12528 [==============================] - 2s 133us/sample - loss: 14.4828 - acc: 0.0916 - val_loss: 14.4817 - val_acc: 0.0916\n",
      "Epoch 10/20\n",
      "12528/12528 [==============================] - 2s 132us/sample - loss: 14.4828 - acc: 0.0916 - val_loss: 14.4817 - val_acc: 0.0916\n",
      "Epoch 11/20\n",
      "12528/12528 [==============================] - 2s 135us/sample - loss: 14.4828 - acc: 0.0916 - val_loss: 14.4817 - val_acc: 0.0916\n",
      "Epoch 12/20\n",
      "12528/12528 [==============================] - 2s 133us/sample - loss: 14.4828 - acc: 0.0916 - val_loss: 14.4817 - val_acc: 0.0916\n",
      "Epoch 13/20\n",
      "12528/12528 [==============================] - 2s 135us/sample - loss: 14.4828 - acc: 0.0916 - val_loss: 14.4817 - val_acc: 0.0916\n",
      "Epoch 14/20\n",
      "12528/12528 [==============================] - 2s 137us/sample - loss: 14.4828 - acc: 0.0916 - val_loss: 14.4817 - val_acc: 0.0916\n",
      "Epoch 15/20\n",
      "12528/12528 [==============================] - 2s 125us/sample - loss: 14.4828 - acc: 0.0916 - val_loss: 14.4817 - val_acc: 0.0916\n",
      "Epoch 16/20\n",
      "12528/12528 [==============================] - 2s 127us/sample - loss: 14.4828 - acc: 0.0916 - val_loss: 14.4817 - val_acc: 0.0916\n",
      "Epoch 17/20\n",
      "12528/12528 [==============================] - 2s 127us/sample - loss: 14.4828 - acc: 0.0916 - val_loss: 14.4817 - val_acc: 0.0916\n",
      "Epoch 18/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12528/12528 [==============================] - 2s 126us/sample - loss: 14.4828 - acc: 0.0916 - val_loss: 14.4817 - val_acc: 0.0916\n",
      "Epoch 19/20\n",
      "12528/12528 [==============================] - 2s 126us/sample - loss: 14.4828 - acc: 0.0916 - val_loss: 14.4817 - val_acc: 0.0916\n",
      "Epoch 20/20\n",
      "12528/12528 [==============================] - 2s 125us/sample - loss: 14.4828 - acc: 0.0916 - val_loss: 14.4817 - val_acc: 0.0916\n"
     ]
    }
   ],
   "source": [
    "model1 = get_model(3, 16)\n",
    "model1_callbacks = model1.fit(X_train, y_train, epochs=20, batch_size=16, validation_data=(X_val, y_val))\n",
    "\n",
    "model2 = get_model(1, 64)\n",
    "model2_callbacks = model2.fit(X_train, y_train, epochs=20, batch_size=16, validation_data=(X_val, y_val))\n",
    "\n",
    "model3 = get_model(4, 8)\n",
    "model3_callbacks = model3.fit(X_train, y_train, epochs=20, batch_size=16, validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.171892</td>\n",
       "      <td>0.957375</td>\n",
       "      <td>0.094762</td>\n",
       "      <td>0.972439</td>\n",
       "      <td>0.248398</td>\n",
       "      <td>0.957934</td>\n",
       "      <td>0.129885</td>\n",
       "      <td>0.962756</td>\n",
       "      <td>14.482781</td>\n",
       "      <td>0.091555</td>\n",
       "      <td>14.481742</td>\n",
       "      <td>0.09162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.102201</td>\n",
       "      <td>0.971105</td>\n",
       "      <td>0.085261</td>\n",
       "      <td>0.975791</td>\n",
       "      <td>0.108265</td>\n",
       "      <td>0.971185</td>\n",
       "      <td>0.103671</td>\n",
       "      <td>0.972067</td>\n",
       "      <td>14.482781</td>\n",
       "      <td>0.091555</td>\n",
       "      <td>14.481742</td>\n",
       "      <td>0.09162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.095519</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.092993</td>\n",
       "      <td>0.975791</td>\n",
       "      <td>0.104561</td>\n",
       "      <td>0.971983</td>\n",
       "      <td>0.090560</td>\n",
       "      <td>0.975419</td>\n",
       "      <td>14.482781</td>\n",
       "      <td>0.091555</td>\n",
       "      <td>14.481742</td>\n",
       "      <td>0.09162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.094226</td>\n",
       "      <td>0.973260</td>\n",
       "      <td>0.089577</td>\n",
       "      <td>0.973557</td>\n",
       "      <td>0.103780</td>\n",
       "      <td>0.972941</td>\n",
       "      <td>0.087189</td>\n",
       "      <td>0.975791</td>\n",
       "      <td>14.482781</td>\n",
       "      <td>0.091555</td>\n",
       "      <td>14.481742</td>\n",
       "      <td>0.09162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.092929</td>\n",
       "      <td>0.972142</td>\n",
       "      <td>0.080464</td>\n",
       "      <td>0.977281</td>\n",
       "      <td>0.103524</td>\n",
       "      <td>0.973340</td>\n",
       "      <td>0.102971</td>\n",
       "      <td>0.975047</td>\n",
       "      <td>14.482781</td>\n",
       "      <td>0.091555</td>\n",
       "      <td>14.481742</td>\n",
       "      <td>0.09162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.089850</td>\n",
       "      <td>0.974298</td>\n",
       "      <td>0.082269</td>\n",
       "      <td>0.978399</td>\n",
       "      <td>0.097313</td>\n",
       "      <td>0.973819</td>\n",
       "      <td>0.088032</td>\n",
       "      <td>0.973929</td>\n",
       "      <td>14.482781</td>\n",
       "      <td>0.091555</td>\n",
       "      <td>14.481742</td>\n",
       "      <td>0.09162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.089072</td>\n",
       "      <td>0.974377</td>\n",
       "      <td>0.082312</td>\n",
       "      <td>0.976909</td>\n",
       "      <td>0.099672</td>\n",
       "      <td>0.973340</td>\n",
       "      <td>0.083855</td>\n",
       "      <td>0.978771</td>\n",
       "      <td>14.482781</td>\n",
       "      <td>0.091555</td>\n",
       "      <td>14.481742</td>\n",
       "      <td>0.09162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.090023</td>\n",
       "      <td>0.975255</td>\n",
       "      <td>0.099980</td>\n",
       "      <td>0.973929</td>\n",
       "      <td>0.095568</td>\n",
       "      <td>0.973180</td>\n",
       "      <td>0.097018</td>\n",
       "      <td>0.972067</td>\n",
       "      <td>14.482781</td>\n",
       "      <td>0.091555</td>\n",
       "      <td>14.481742</td>\n",
       "      <td>0.09162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.088698</td>\n",
       "      <td>0.974697</td>\n",
       "      <td>0.078332</td>\n",
       "      <td>0.979516</td>\n",
       "      <td>0.096170</td>\n",
       "      <td>0.973020</td>\n",
       "      <td>0.097322</td>\n",
       "      <td>0.976164</td>\n",
       "      <td>14.482781</td>\n",
       "      <td>0.091555</td>\n",
       "      <td>14.481742</td>\n",
       "      <td>0.09162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.085959</td>\n",
       "      <td>0.974777</td>\n",
       "      <td>0.082081</td>\n",
       "      <td>0.978399</td>\n",
       "      <td>0.096453</td>\n",
       "      <td>0.974218</td>\n",
       "      <td>0.090788</td>\n",
       "      <td>0.973557</td>\n",
       "      <td>14.482781</td>\n",
       "      <td>0.091555</td>\n",
       "      <td>14.481742</td>\n",
       "      <td>0.09162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.084995</td>\n",
       "      <td>0.976373</td>\n",
       "      <td>0.078455</td>\n",
       "      <td>0.976909</td>\n",
       "      <td>0.093903</td>\n",
       "      <td>0.973659</td>\n",
       "      <td>0.094164</td>\n",
       "      <td>0.976164</td>\n",
       "      <td>14.482781</td>\n",
       "      <td>0.091555</td>\n",
       "      <td>14.481742</td>\n",
       "      <td>0.09162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.084253</td>\n",
       "      <td>0.975655</td>\n",
       "      <td>0.077042</td>\n",
       "      <td>0.976909</td>\n",
       "      <td>0.091234</td>\n",
       "      <td>0.975415</td>\n",
       "      <td>0.078002</td>\n",
       "      <td>0.976909</td>\n",
       "      <td>14.482781</td>\n",
       "      <td>0.091555</td>\n",
       "      <td>14.481742</td>\n",
       "      <td>0.09162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.083694</td>\n",
       "      <td>0.974697</td>\n",
       "      <td>0.079937</td>\n",
       "      <td>0.976909</td>\n",
       "      <td>0.088470</td>\n",
       "      <td>0.977490</td>\n",
       "      <td>0.082497</td>\n",
       "      <td>0.978399</td>\n",
       "      <td>14.482781</td>\n",
       "      <td>0.091555</td>\n",
       "      <td>14.481742</td>\n",
       "      <td>0.09162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.084813</td>\n",
       "      <td>0.975734</td>\n",
       "      <td>0.077890</td>\n",
       "      <td>0.978026</td>\n",
       "      <td>0.091499</td>\n",
       "      <td>0.975016</td>\n",
       "      <td>0.083721</td>\n",
       "      <td>0.978399</td>\n",
       "      <td>14.482781</td>\n",
       "      <td>0.091555</td>\n",
       "      <td>14.481742</td>\n",
       "      <td>0.09162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.082774</td>\n",
       "      <td>0.975575</td>\n",
       "      <td>0.079670</td>\n",
       "      <td>0.975047</td>\n",
       "      <td>0.090982</td>\n",
       "      <td>0.975176</td>\n",
       "      <td>0.087126</td>\n",
       "      <td>0.978771</td>\n",
       "      <td>14.482781</td>\n",
       "      <td>0.091555</td>\n",
       "      <td>14.481742</td>\n",
       "      <td>0.09162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.081960</td>\n",
       "      <td>0.976772</td>\n",
       "      <td>0.080109</td>\n",
       "      <td>0.975791</td>\n",
       "      <td>0.089077</td>\n",
       "      <td>0.975575</td>\n",
       "      <td>0.082540</td>\n",
       "      <td>0.978771</td>\n",
       "      <td>14.482781</td>\n",
       "      <td>0.091555</td>\n",
       "      <td>14.481742</td>\n",
       "      <td>0.09162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.082051</td>\n",
       "      <td>0.975974</td>\n",
       "      <td>0.076938</td>\n",
       "      <td>0.978026</td>\n",
       "      <td>0.088157</td>\n",
       "      <td>0.976293</td>\n",
       "      <td>0.080980</td>\n",
       "      <td>0.978399</td>\n",
       "      <td>14.482781</td>\n",
       "      <td>0.091555</td>\n",
       "      <td>14.481742</td>\n",
       "      <td>0.09162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.082133</td>\n",
       "      <td>0.976054</td>\n",
       "      <td>0.077907</td>\n",
       "      <td>0.978399</td>\n",
       "      <td>0.087910</td>\n",
       "      <td>0.976772</td>\n",
       "      <td>0.080698</td>\n",
       "      <td>0.977281</td>\n",
       "      <td>14.482781</td>\n",
       "      <td>0.091555</td>\n",
       "      <td>14.481742</td>\n",
       "      <td>0.09162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.081501</td>\n",
       "      <td>0.976692</td>\n",
       "      <td>0.078779</td>\n",
       "      <td>0.978399</td>\n",
       "      <td>0.088379</td>\n",
       "      <td>0.975575</td>\n",
       "      <td>0.096980</td>\n",
       "      <td>0.973557</td>\n",
       "      <td>14.482781</td>\n",
       "      <td>0.091555</td>\n",
       "      <td>14.481742</td>\n",
       "      <td>0.09162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.083086</td>\n",
       "      <td>0.976692</td>\n",
       "      <td>0.076321</td>\n",
       "      <td>0.977281</td>\n",
       "      <td>0.085393</td>\n",
       "      <td>0.976133</td>\n",
       "      <td>0.078179</td>\n",
       "      <td>0.976909</td>\n",
       "      <td>14.482781</td>\n",
       "      <td>0.091555</td>\n",
       "      <td>14.481742</td>\n",
       "      <td>0.09162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss       acc  val_loss   val_acc      loss       acc  val_loss  \\\n",
       "0   0.171892  0.957375  0.094762  0.972439  0.248398  0.957934  0.129885   \n",
       "1   0.102201  0.971105  0.085261  0.975791  0.108265  0.971185  0.103671   \n",
       "2   0.095519  0.972222  0.092993  0.975791  0.104561  0.971983  0.090560   \n",
       "3   0.094226  0.973260  0.089577  0.973557  0.103780  0.972941  0.087189   \n",
       "4   0.092929  0.972142  0.080464  0.977281  0.103524  0.973340  0.102971   \n",
       "5   0.089850  0.974298  0.082269  0.978399  0.097313  0.973819  0.088032   \n",
       "6   0.089072  0.974377  0.082312  0.976909  0.099672  0.973340  0.083855   \n",
       "7   0.090023  0.975255  0.099980  0.973929  0.095568  0.973180  0.097018   \n",
       "8   0.088698  0.974697  0.078332  0.979516  0.096170  0.973020  0.097322   \n",
       "9   0.085959  0.974777  0.082081  0.978399  0.096453  0.974218  0.090788   \n",
       "10  0.084995  0.976373  0.078455  0.976909  0.093903  0.973659  0.094164   \n",
       "11  0.084253  0.975655  0.077042  0.976909  0.091234  0.975415  0.078002   \n",
       "12  0.083694  0.974697  0.079937  0.976909  0.088470  0.977490  0.082497   \n",
       "13  0.084813  0.975734  0.077890  0.978026  0.091499  0.975016  0.083721   \n",
       "14  0.082774  0.975575  0.079670  0.975047  0.090982  0.975176  0.087126   \n",
       "15  0.081960  0.976772  0.080109  0.975791  0.089077  0.975575  0.082540   \n",
       "16  0.082051  0.975974  0.076938  0.978026  0.088157  0.976293  0.080980   \n",
       "17  0.082133  0.976054  0.077907  0.978399  0.087910  0.976772  0.080698   \n",
       "18  0.081501  0.976692  0.078779  0.978399  0.088379  0.975575  0.096980   \n",
       "19  0.083086  0.976692  0.076321  0.977281  0.085393  0.976133  0.078179   \n",
       "\n",
       "     val_acc       loss       acc   val_loss  val_acc  \n",
       "0   0.962756  14.482781  0.091555  14.481742  0.09162  \n",
       "1   0.972067  14.482781  0.091555  14.481742  0.09162  \n",
       "2   0.975419  14.482781  0.091555  14.481742  0.09162  \n",
       "3   0.975791  14.482781  0.091555  14.481742  0.09162  \n",
       "4   0.975047  14.482781  0.091555  14.481742  0.09162  \n",
       "5   0.973929  14.482781  0.091555  14.481742  0.09162  \n",
       "6   0.978771  14.482781  0.091555  14.481742  0.09162  \n",
       "7   0.972067  14.482781  0.091555  14.481742  0.09162  \n",
       "8   0.976164  14.482781  0.091555  14.481742  0.09162  \n",
       "9   0.973557  14.482781  0.091555  14.481742  0.09162  \n",
       "10  0.976164  14.482781  0.091555  14.481742  0.09162  \n",
       "11  0.976909  14.482781  0.091555  14.481742  0.09162  \n",
       "12  0.978399  14.482781  0.091555  14.481742  0.09162  \n",
       "13  0.978399  14.482781  0.091555  14.481742  0.09162  \n",
       "14  0.978771  14.482781  0.091555  14.481742  0.09162  \n",
       "15  0.978771  14.482781  0.091555  14.481742  0.09162  \n",
       "16  0.978399  14.482781  0.091555  14.481742  0.09162  \n",
       "17  0.977281  14.482781  0.091555  14.481742  0.09162  \n",
       "18  0.973557  14.482781  0.091555  14.481742  0.09162  \n",
       "19  0.976909  14.482781  0.091555  14.481742  0.09162  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([pd.DataFrame(model1_callbacks.history), pd.DataFrame(model2_callbacks.history), pd.DataFrame(model3_callbacks.history)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Model 3 has the lowest validation loss and high validation accuracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2685/2685 [==============================] - 0s 22us/sample - loss: 14.4817 - acc: 0.0916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[14.481740873041987, 0.09162011]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.evaluate(X_tes, y_tes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_30 (Dense)             (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 297\n",
      "Trainable params: 297\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00      2439\n",
      "         1.0       0.09      1.00      0.17       246\n",
      "\n",
      "    accuracy                           0.09      2685\n",
      "   macro avg       0.05      0.50      0.08      2685\n",
      "weighted avg       0.01      0.09      0.02      2685\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\00004891\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "preds = model3.predict_classes(X_tes)\n",
    "print(classification_report(y_tes, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0 2439]\n",
      " [   0  246]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_tes, preds.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    2439\n",
       "1.0     246\n",
       "dtype: int64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_tes).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2685\n",
       "dtype: int64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(preds.flatten()).value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
